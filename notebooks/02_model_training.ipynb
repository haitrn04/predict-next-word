{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Huấn luyện Mô hình N-gram và LSTM\n",
    "## Dự đoán từ tiếp theo - Next Word Prediction\n",
    "\n",
    "Notebook này thực hiện:\n",
    "1. Load dữ liệu đã tiền xử lý\n",
    "2. Xây dựng mô hình N-gram từ đầu (không dùng thư viện)\n",
    "3. Xây dựng mô hình LSTM từ đầu (không dùng PyTorch/TensorFlow)\n",
    "4. Training và đánh giá\n",
    "5. Lưu mô hình dưới dạng file .pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import thư viện và Load dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thư viện đã được import!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"Thư viện đã được import!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang load dữ liệu...\n",
      "✓ Vocabulary size: 54822\n",
      "✓ Training data - X shape: (4199877, 10), y shape: (4199877,)\n",
      "✓ Tokenized texts: 210509 documents\n",
      "\n",
      "Config: {'num_documents': 10000, 'num_cleaned_documents': 10000, 'num_sentences': 210509, 'num_sequences': 210509, 'vocab_size': 54822, 'max_seq_len': 10, 'min_word_freq': 2, 'num_training_pairs': 4199877, 'preprocessing_method': 'hierarchical'}\n"
     ]
    }
   ],
   "source": [
    "# Load vocabulary\n",
    "print(\"Đang load dữ liệu...\")\n",
    "with open('../data/processed/vocabulary.pkl', 'rb') as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "    vocab = vocab_data['vocab']\n",
    "    word2idx = vocab_data['word2idx']\n",
    "    idx2word = vocab_data['idx2word']\n",
    "    vocab_size = vocab_data['vocab_size']\n",
    "\n",
    "print(f\"✓ Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Load training data\n",
    "with open('../data/processed/training_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    X = train_data['X']\n",
    "    y = train_data['y']\n",
    "    max_seq_len = train_data['max_seq_len']\n",
    "\n",
    "print(f\"✓ Training data - X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Load tokenized texts (cho n-gram)\n",
    "with open('../data/processed/tokenized_texts.pkl', 'rb') as f:\n",
    "    tokenized_texts = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Tokenized texts: {len(tokenized_texts)} documents\")\n",
    "\n",
    "# Load config\n",
    "with open('../data/processed/config.pkl', 'rb') as f:\n",
    "    config = pickle.load(f)\n",
    "\n",
    "print(f\"\\nConfig: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mô hình N-gram (Tự xây dựng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramModel:\n",
    "    \"\"\"\n",
    "    Mô hình N-gram được xây dựng từ đầu\n",
    "    Sử dụng smoothing để xử lý các n-gram chưa thấy\n",
    "    \"\"\"\n",
    "    def __init__(self, n=3, smoothing=0.01):\n",
    "        self.n = n\n",
    "        self.smoothing = smoothing\n",
    "        self.ngram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        \n",
    "    def train(self, tokenized_texts):\n",
    "        \"\"\"\n",
    "        Huấn luyện mô hình n-gram từ dữ liệu tokenized\n",
    "        \"\"\"\n",
    "        print(f\"Training {self.n}-gram model...\")\n",
    "        \n",
    "        for text in tokenized_texts:\n",
    "            # Thêm tokens vào vocabulary\n",
    "            self.vocab.update(text)\n",
    "            \n",
    "            # Tạo n-grams\n",
    "            for i in range(len(text) - self.n + 1):\n",
    "                # Lấy n-1 từ làm context\n",
    "                context = tuple(text[i:i+self.n-1])\n",
    "                # Từ tiếp theo\n",
    "                next_word = text[i+self.n-1]\n",
    "                \n",
    "                # Đếm\n",
    "                self.ngram_counts[context][next_word] += 1\n",
    "                self.context_counts[context] += 1\n",
    "        \n",
    "        print(f\"Trained on {len(tokenized_texts)} documents\")\n",
    "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"Unique contexts: {len(self.context_counts)}\")\n",
    "    \n",
    "    def get_probabilities(self, context):\n",
    "        \"\"\"\n",
    "        Tính xác suất cho từng từ tiếp theo dựa trên context\n",
    "        Sử dụng Laplace smoothing\n",
    "        \"\"\"\n",
    "        context = tuple(context)\n",
    "        \n",
    "        # Số lần context xuất hiện\n",
    "        context_count = self.context_counts[context]\n",
    "        \n",
    "        # Tính xác suất cho mỗi từ trong vocab\n",
    "        probabilities = {}\n",
    "        vocab_size = len(self.vocab)\n",
    "        \n",
    "        if context_count > 0:\n",
    "            # Context đã thấy trong training\n",
    "            for word in self.vocab:\n",
    "                word_count = self.ngram_counts[context].get(word, 0)\n",
    "                # Laplace smoothing\n",
    "                prob = (word_count + self.smoothing) / (context_count + self.smoothing * vocab_size)\n",
    "                probabilities[word] = prob\n",
    "        else:\n",
    "            # Context chưa thấy - uniform distribution\n",
    "            uniform_prob = 1.0 / vocab_size\n",
    "            for word in self.vocab:\n",
    "                probabilities[word] = uniform_prob\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def predict_top_k(self, context, k=5):\n",
    "        \"\"\"\n",
    "        Dự đoán top k từ tiếp theo\n",
    "        \"\"\"\n",
    "        # Lấy n-1 từ cuối làm context\n",
    "        if len(context) >= self.n - 1:\n",
    "            context = context[-(self.n-1):]\n",
    "        \n",
    "        # Tính xác suất\n",
    "        probs = self.get_probabilities(context)\n",
    "        \n",
    "        # Sort và lấy top k\n",
    "        sorted_words = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return [(word, prob) for word, prob in sorted_words[:k]]\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"\n",
    "        Lưu mô hình\n",
    "        \"\"\"\n",
    "        model_data = {\n",
    "            'n': self.n,\n",
    "            'smoothing': self.smoothing,\n",
    "            'ngram_counts': dict(self.ngram_counts),\n",
    "            'context_counts': dict(self.context_counts),\n",
    "            'vocab': list(self.vocab)\n",
    "        }\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        \"\"\"\n",
    "        Load mô hình\n",
    "        \"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        model = NgramModel(n=model_data['n'], smoothing=model_data['smoothing'])\n",
    "        model.ngram_counts = defaultdict(lambda: defaultdict(int), model_data['ngram_counts'])\n",
    "        model.context_counts = defaultdict(int, model_data['context_counts'])\n",
    "        model.vocab = set(model_data['vocab'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING N-GRAM MODEL\n",
      "============================================================\n",
      "Training 3-gram model...\n",
      "Trained on 210509 documents\n",
      "Vocabulary size: 125859\n",
      "Unique contexts: 1265991\n",
      "\n",
      "✓ N-gram model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train mô hình Trigram (n=3)\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING N-GRAM MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ngram_model = NgramModel(n=3, smoothing=0.01)\n",
    "ngram_model.train(tokenized_texts)\n",
    "\n",
    "print(\"\\n✓ N-gram model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test N-gram model:\n",
      "\n",
      "Context: ['tôi', 'đi']\n",
      "Top 5 predictions:\n",
      "  làm: 0.0059\n",
      "  ăn: 0.0037\n",
      "  dạo: 0.0022\n",
      "  qua: 0.0022\n",
      "  theo: 0.0022\n",
      "\n",
      "Context: ['bài', 'báo', 'này', 'nói']\n",
      "Top 5 predictions:\n",
      "  rằng: 0.0069\n",
      "  với: 0.0031\n",
      "  thêm: 0.0031\n",
      "  về: 0.0023\n",
      "  và: 0.0015\n"
     ]
    }
   ],
   "source": [
    "# Test N-gram model\n",
    "print(\"\\nTest N-gram model:\")\n",
    "test_contexts = [\n",
    "    ['tôi', 'đi'],\n",
    "\n",
    "    ['bài', 'báo', 'này', 'nói']\n",
    "]\n",
    "\n",
    "for context in test_contexts:\n",
    "    predictions = ngram_model.predict_top_k(context, k=5)\n",
    "    print(f\"\\nContext: {context}\")\n",
    "    print(\"Top 5 predictions:\")\n",
    "    for word, prob in predictions:\n",
    "        print(f\"  {word}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mô hình LSTM (Tự xây dựng từ NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Tanh activation function\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax activation function\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "class SimpleLSTM:\n",
    "    \"\"\"\n",
    "    LSTM đơn giản được xây dựng từ đầu bằng NumPy\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim=50, hidden_dim=128, max_seq_len=10):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Khởi tạo weights với Xavier initialization\"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = np.random.randn(self.vocab_size, self.embedding_dim) * 0.01\n",
    "        \n",
    "        # LSTM weights - simplified version\n",
    "        # Forget gate\n",
    "        self.Wf = np.random.randn(self.embedding_dim + self.hidden_dim, self.hidden_dim) * 0.01\n",
    "        self.bf = np.zeros((1, self.hidden_dim))\n",
    "        \n",
    "        # Input gate\n",
    "        self.Wi = np.random.randn(self.embedding_dim + self.hidden_dim, self.hidden_dim) * 0.01\n",
    "        self.bi = np.zeros((1, self.hidden_dim))\n",
    "        \n",
    "        # Cell gate\n",
    "        self.Wc = np.random.randn(self.embedding_dim + self.hidden_dim, self.hidden_dim) * 0.01\n",
    "        self.bc = np.zeros((1, self.hidden_dim))\n",
    "        \n",
    "        # Output gate\n",
    "        self.Wo = np.random.randn(self.embedding_dim + self.hidden_dim, self.hidden_dim) * 0.01\n",
    "        self.bo = np.zeros((1, self.hidden_dim))\n",
    "        \n",
    "        # Output layer\n",
    "        self.Wy = np.random.randn(self.hidden_dim, self.vocab_size) * 0.01\n",
    "        self.by = np.zeros((1, self.vocab_size))\n",
    "        \n",
    "    def forward_step(self, x_t, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Forward pass cho một timestep\n",
    "        \"\"\"\n",
    "        # Concatenate input and previous hidden state\n",
    "        combined = np.concatenate([x_t, h_prev], axis=1)\n",
    "        \n",
    "        # LSTM gates\n",
    "        ft = sigmoid(np.dot(combined, self.Wf) + self.bf)  # Forget gate\n",
    "        it = sigmoid(np.dot(combined, self.Wi) + self.bi)  # Input gate\n",
    "        c_tilde = tanh(np.dot(combined, self.Wc) + self.bc)  # Candidate cell state\n",
    "        c_t = ft * c_prev + it * c_tilde  # New cell state\n",
    "        ot = sigmoid(np.dot(combined, self.Wo) + self.bo)  # Output gate\n",
    "        h_t = ot * tanh(c_t)  # New hidden state\n",
    "        \n",
    "        return h_t, c_t\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass cho cả sequence\n",
    "        X shape: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        seq_len = X.shape[1]\n",
    "        \n",
    "        # Initialize hidden and cell states\n",
    "        h = np.zeros((batch_size, self.hidden_dim))\n",
    "        c = np.zeros((batch_size, self.hidden_dim))\n",
    "        \n",
    "        # Process sequence\n",
    "        for t in range(seq_len):\n",
    "            # Get embeddings for current timestep\n",
    "            x_t = self.embedding[X[:, t]]  # (batch_size, embedding_dim)\n",
    "            \n",
    "            # LSTM step\n",
    "            h, c = self.forward_step(x_t, h, c)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = np.dot(h, self.Wy) + self.by\n",
    "        probs = softmax(logits)\n",
    "        \n",
    "        return probs, h\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Dự đoán từ tiếp theo\n",
    "        \"\"\"\n",
    "        probs, _ = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def predict_top_k(self, X, k=5):\n",
    "        \"\"\"\n",
    "        Dự đoán top k từ tiếp theo\n",
    "        \"\"\"\n",
    "        probs, _ = self.forward(X)\n",
    "        \n",
    "        # Get top k indices\n",
    "        top_k_indices = np.argsort(probs, axis=1)[:, -k:][:, ::-1]\n",
    "        top_k_probs = np.take_along_axis(probs, top_k_indices, axis=1)\n",
    "        \n",
    "        return top_k_indices, top_k_probs\n",
    "    \n",
    "    def compute_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Tính cross-entropy loss\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        probs, _ = self.forward(X)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        correct_probs = probs[np.arange(batch_size), y]\n",
    "        loss = -np.mean(np.log(correct_probs + 1e-10))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train_step(self, X_batch, y_batch, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Training step đơn giản với gradient descent\n",
    "        (Simplified - không implement full backprop through time)\n",
    "        \"\"\"\n",
    "        batch_size = X_batch.shape[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        probs, h = self.forward(X_batch)\n",
    "        \n",
    "        # Compute gradients (simplified)\n",
    "        # Gradient of loss w.r.t output\n",
    "        dL_dy = probs.copy()\n",
    "        dL_dy[np.arange(batch_size), y_batch] -= 1\n",
    "        dL_dy /= batch_size\n",
    "        \n",
    "        # Update output layer\n",
    "        dWy = np.dot(h.T, dL_dy)\n",
    "        dby = np.sum(dL_dy, axis=0, keepdims=True)\n",
    "        \n",
    "        self.Wy -= learning_rate * dWy\n",
    "        self.by -= learning_rate * dby\n",
    "        \n",
    "        # Update embeddings (simplified)\n",
    "        for i in range(batch_size):\n",
    "            for t in range(X_batch.shape[1]):\n",
    "                idx = X_batch[i, t]\n",
    "                if idx > 0:  # Skip padding\n",
    "                    self.embedding[idx] -= learning_rate * 0.001 * np.random.randn(self.embedding_dim)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(X_batch, y_batch)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"\n",
    "        Lưu mô hình\n",
    "        \"\"\"\n",
    "        model_data = {\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'hidden_dim': self.hidden_dim,\n",
    "            'max_seq_len': self.max_seq_len,\n",
    "            'embedding': self.embedding,\n",
    "            'Wf': self.Wf, 'bf': self.bf,\n",
    "            'Wi': self.Wi, 'bi': self.bi,\n",
    "            'Wc': self.Wc, 'bc': self.bc,\n",
    "            'Wo': self.Wo, 'bo': self.bo,\n",
    "            'Wy': self.Wy, 'by': self.by\n",
    "        }\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        \"\"\"\n",
    "        Load mô hình\n",
    "        \"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        model = SimpleLSTM(\n",
    "            vocab_size=model_data['vocab_size'],\n",
    "            embedding_dim=model_data['embedding_dim'],\n",
    "            hidden_dim=model_data['hidden_dim'],\n",
    "            max_seq_len=model_data['max_seq_len']\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        model.embedding = model_data['embedding']\n",
    "        model.Wf = model_data['Wf']\n",
    "        model.bf = model_data['bf']\n",
    "        model.Wi = model_data['Wi']\n",
    "        model.bi = model_data['bi']\n",
    "        model.Wc = model_data['Wc']\n",
    "        model.bc = model_data['bc']\n",
    "        model.Wo = model_data['Wo']\n",
    "        model.bo = model_data['bo']\n",
    "        model.Wy = model_data['Wy']\n",
    "        model.by = model_data['by']\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING LSTM MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Khởi tạo model\n",
    "lstm_model = SimpleLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=50,\n",
    "    hidden_dim=128,\n",
    "    max_seq_len=max_seq_len\n",
    ")\n",
    "\n",
    "print(f\"\\nModel initialized:\")\n",
    "print(f\"  Vocab size: {vocab_size}\")\n",
    "print(f\"  Embedding dim: 50\")\n",
    "print(f\"  Hidden dim: 128\")\n",
    "print(f\"  Max sequence length: {max_seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia train/validation\n",
    "train_size = int(0.9 * len(X))\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(f\"\\nTraining for {epochs} epochs...\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_train_shuffled = X_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "    \n",
    "    # Training\n",
    "    total_loss = 0\n",
    "    num_batches = len(X_train) // batch_size\n",
    "    \n",
    "    for i in range(0, len(X_train_shuffled), batch_size):\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_shuffled[i:i+batch_size]\n",
    "        \n",
    "        if len(X_batch) < batch_size:\n",
    "            continue\n",
    "        \n",
    "        loss = lstm_model.train_step(X_batch, y_batch, learning_rate)\n",
    "        total_loss += loss\n",
    "        \n",
    "        if (i // batch_size) % 50 == 0:\n",
    "            print(f\"  Batch {i//batch_size}/{num_batches}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = lstm_model.compute_loss(X_val[:1000], y_val[:1000])\n",
    "    \n",
    "    print(f\"  Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"\\n✓ LSTM model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LSTM model\n",
    "print(\"\\nTest LSTM model:\")\n",
    "test_samples = X_val[:5]\n",
    "\n",
    "for i, sample in enumerate(test_samples):\n",
    "    # Decode input\n",
    "    input_words = [idx2word[idx] for idx in sample if idx != word2idx['<PAD>']]\n",
    "    \n",
    "    # Predict\n",
    "    top_k_indices, top_k_probs = lstm_model.predict_top_k(sample.reshape(1, -1), k=5)\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Input: {' '.join(input_words)}\")\n",
    "    print(\"Top 5 predictions:\")\n",
    "    for idx, prob in zip(top_k_indices[0], top_k_probs[0]):\n",
    "        print(f\"  {idx2word[idx]}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Đánh giá mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_top_k(model, X, y, k=5, model_type='lstm'):\n",
    "    \"\"\"\n",
    "    Tính accuracy top-k\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = len(y)\n",
    "    \n",
    "    # Process in batches for LSTM\n",
    "    batch_size = 100\n",
    "    \n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "        \n",
    "        if model_type == 'lstm':\n",
    "            top_k_indices, _ = model.predict_top_k(X_batch, k=k)\n",
    "            \n",
    "            for j, true_idx in enumerate(y_batch):\n",
    "                if true_idx in top_k_indices[j]:\n",
    "                    correct += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Đánh giá LSTM\n",
    "print(\"\\nĐánh giá LSTM model:\")\n",
    "val_subset = 1000\n",
    "for k in [1, 3, 5]:\n",
    "    acc = calculate_accuracy_top_k(lstm_model, X_val[:val_subset], y_val[:val_subset], k=k, model_type='lstm')\n",
    "    print(f\"  Top-{k} Accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lưu mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo thư mục models\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "print(\"Đang lưu mô hình...\")\n",
    "\n",
    "# Lưu N-gram model\n",
    "ngram_model.save('../models/ngram_model.pkl')\n",
    "print(\"✓ N-gram model saved\")\n",
    "\n",
    "# Lưu LSTM model\n",
    "lstm_model.save('../models/lstm_model.pkl')\n",
    "print(\"✓ LSTM model saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HOÀN THÀNH TRAINING!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Các mô hình đã được lưu trong thư mục: ../models/\")\n",
    "print(\"\\nCác file:\")\n",
    "print(\"  - ngram_model.pkl: Mô hình N-gram\")\n",
    "print(\"  - lstm_model.pkl: Mô hình LSTM\")\n",
    "print(\"\\nBạn có thể chuyển sang notebook tiếp theo để test mô hình!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
