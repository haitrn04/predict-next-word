# HÆ°á»›ng dáº«n Nhanh - Next Word Prediction

## Báº¯t Ä‘áº§u nhanh trong 3 bÆ°á»›c

### BÆ°á»›c 1: CÃ i Ä‘áº·t thÆ° viá»‡n

```bash
pip install -r requirements.txt
```

Hoáº·c cÃ i thá»§ cÃ´ng:

```bash
pip install datasets underthesea numpy jupyter
```

### BÆ°á»›c 2: Cháº¡y cÃ¡c notebooks theo thá»© tá»±

#### 2.1. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u

```bash
jupyter notebook notebooks/01_data_preprocessing.ipynb
```

Cháº¡y táº¥t cáº£ cÃ¡c cell (Cell â†’ Run All). Notebook nÃ y sáº½:
- Táº£i 10,000 dÃ²ng tá»« VTSNLP dataset
- Tiá»n xá»­ lÃ½ vÃ  tokenize
- LÆ°u vÃ o `data/processed/`

**Thá»i gian:** ~5-10 phÃºt

#### 2.2. Huáº¥n luyá»‡n mÃ´ hÃ¬nh

```bash
jupyter notebook notebooks/02_model_training.ipynb
```

Cháº¡y táº¥t cáº£ cÃ¡c cell. Notebook nÃ y sáº½:
- Train mÃ´ hÃ¬nh N-gram
- Train mÃ´ hÃ¬nh LSTM (5 epochs)
- LÆ°u vÃ o `models/`

**Thá»i gian:** ~10-30 phÃºt (tÃ¹y cáº¥u hÃ¬nh mÃ¡y)

#### 2.3. Test mÃ´ hÃ¬nh

```bash
jupyter notebook notebooks/03_model_testing.ipynb
```

Cháº¡y táº¥t cáº£ cÃ¡c cell Ä‘á»ƒ test mÃ´ hÃ¬nh.

### BÆ°á»›c 3: Sá»­ dá»¥ng

Trong notebook 03, sá»­ dá»¥ng hÃ m `predict_next_word()`:

```python
# VÃ­ dá»¥ Ä‘Æ¡n giáº£n
input_text = "tÃ´i Ä‘i há»c báº±ng "
predictions = predict_next_word(input_text, 3)
print(predictions)
```

## VÃ­ dá»¥ sá»­ dá»¥ng chi tiáº¿t

### VÃ­ dá»¥ 1: Dá»± Ä‘oÃ¡n vá»›i LSTM

```python
input_text = "tÃ´i Ä‘i há»c báº±ng "
result = predict_next_word(input_text, top_k=3, model_type='lstm')
print(result)
# Output: ['xe', 'Ä‘Æ°á»ng', 'cÃ¡ch']
```

### VÃ­ dá»¥ 2: Dá»± Ä‘oÃ¡n vá»›i N-gram

```python
input_text = "hÃ´m nay trá»i Ä‘áº¹p "
result = predict_next_word(input_text, top_k=5, model_type='ngram')
print(result)
# Output: ['quÃ¡', 'láº¯m', 'tháº­t', 'nhÆ°', 'váº­y']
```

### VÃ­ dá»¥ 3: So sÃ¡nh cáº£ 2 mÃ´ hÃ¬nh

```python
input_text = "viá»‡t nam lÃ  "
result = predict_next_word(input_text, top_k=3, model_type='both')

print("N-gram:", result['ngram'])
print("LSTM:", result['lstm'])
```

### VÃ­ dá»¥ 4: Nhiá»u test cases

```python
test_cases = [
    "tÃ´i thÃ­ch Äƒn ",
    "chÃºng tÃ´i Ä‘ang ",
    "há»c sinh Ä‘i ",
]

for test in test_cases:
    predictions = predict_next_word(test, 3, model_type='lstm')
    print(f"{test} â†’ {predictions}")
```

## Format theo yÃªu cáº§u

```python
input = "tÃ´i Ä‘i há»c báº±ng "
print(predict_next_word(input, 3))
# Output: ['xe Ä‘áº¡p', 'xe buÃ½t', 'Ä‘i bá»™']  # VÃ­ dá»¥
```

## Xá»­ lÃ½ lá»—i thÆ°á»ng gáº·p

### Lá»—i 1: Module not found

```bash
pip install datasets underthesea numpy
```

### Lá»—i 2: File not found (vocabulary.pkl, models, etc.)

â†’ Cháº¡y láº¡i notebook 01 vÃ  02 theo thá»© tá»±

### Lá»—i 3: CUDA/GPU errors

â†’ MÃ´ hÃ¬nh chá»‰ dÃ¹ng NumPy, khÃ´ng cáº§n GPU

### Lá»—i 4: Memory error khi load dataset

â†’ Giáº£m NUM_SAMPLES trong notebook 01 (vÃ­ dá»¥: 5000 thay vÃ¬ 10000)

## TÃ¹y chá»‰nh

### Thay Ä‘á»•i sá»‘ lÆ°á»£ng dá»¯ liá»‡u

Trong `01_data_preprocessing.ipynb`:

```python
NUM_SAMPLES = 5000  # Thay Ä‘á»•i tá»« 10000
```

### Thay Ä‘á»•i tham sá»‘ mÃ´ hÃ¬nh

Trong `02_model_training.ipynb`:

```python
# N-gram
ngram_model = NgramModel(n=4, smoothing=0.01)  # Thay n=3 thÃ nh n=4

# LSTM
lstm_model = SimpleLSTM(
    vocab_size=vocab_size,
    embedding_dim=100,  # TÄƒng tá»« 50
    hidden_dim=256,     # TÄƒng tá»« 128
    max_seq_len=max_seq_len
)
```

### Thay Ä‘á»•i training parameters

```python
epochs = 10          # TÄƒng tá»« 5
batch_size = 64      # Giáº£m tá»« 128
learning_rate = 0.001  # Giáº£m tá»« 0.01
```

## Cáº¥u trÃºc thÆ° má»¥c sau khi hoÃ n thÃ nh

```
nwp/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 02_model_training.ipynb
â”‚   â””â”€â”€ 03_model_testing.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ vocabulary.pkl          â† Tá»« Ä‘iá»ƒn
â”‚       â”œâ”€â”€ training_data.pkl       â† Dá»¯ liá»‡u train
â”‚       â”œâ”€â”€ tokenized_texts.pkl     â† VÄƒn báº£n Ä‘Ã£ token
â”‚       â””â”€â”€ config.pkl              â† Cáº¥u hÃ¬nh
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ngram_model.pkl            â† MÃ´ hÃ¬nh N-gram
â”‚   â””â”€â”€ lstm_model.pkl             â† MÃ´ hÃ¬nh LSTM
â”œâ”€â”€ README.md
â”œâ”€â”€ QUICKSTART.md
â””â”€â”€ requirements.txt
```

## Tips

1. **Cháº¡y notebook theo thá»© tá»±**: 01 â†’ 02 â†’ 03
2. **LÆ°u output**: Sau má»—i notebook, kiá»ƒm tra file Ä‘Ã£ Ä‘Æ°á»£c táº¡o
3. **RAM**: Cáº§n Ã­t nháº¥t 4GB RAM Ä‘á»ƒ cháº¡y thoáº£i mÃ¡i
4. **Thá»i gian**: Tá»•ng thá»i gian ~30-60 phÃºt cho toÃ n bá»™ quÃ¡ trÃ¬nh

## CÃ¢u há»i thÆ°á»ng gáº·p

**Q: TÃ´i cÃ³ thá»ƒ dÃ¹ng dataset khÃ¡c khÃ´ng?**
A: CÃ³, chá»‰nh sá»­a pháº§n load dataset trong notebook 01.

**Q: LÃ m sao Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c?**
A:
- TÄƒng sá»‘ lÆ°á»£ng dá»¯ liá»‡u
- TÄƒng sá»‘ epochs
- TÄƒng kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh (embedding_dim, hidden_dim)
- Äiá»u chá»‰nh learning rate

**Q: MÃ´ hÃ¬nh nÃ o tá»‘t hÆ¡n?**
A:
- N-gram: Nhanh, tá»‘t cho ngá»¯ cáº£nh ngáº¯n
- LSTM: Cháº­m hÆ¡n, tá»‘t cho ngá»¯ cáº£nh dÃ i vÃ  phá»©c táº¡p

**Q: CÃ³ thá»ƒ deploy lÃªn production khÃ´ng?**
A: Code hiá»‡n táº¡i chá»‰ Ä‘á»ƒ há»c táº­p. Äá»ƒ production:
- DÃ¹ng framework chuyÃªn nghiá»‡p (PyTorch/TensorFlow)
- Tá»‘i Æ°u hÃ³a inference
- ThÃªm error handling
- XÃ¢y dá»±ng API

## LiÃªn há»‡ & ÄÃ³ng gÃ³p

Náº¿u gáº·p váº¥n Ä‘á», vui lÃ²ng táº¡o issue hoáº·c liÃªn há»‡.

---

**ChÃºc báº¡n thÃ nh cÃ´ng!** ğŸ‰
